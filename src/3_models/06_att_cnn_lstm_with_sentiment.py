"""
06_att_cnn_lstm_with_sentiment.py

åŠŸèƒ½ï¼š
- ä½¿ç”¨ ä»·æ ¼ç‰¹å¾ + æƒ…ç»ªç‰¹å¾
- æ¨¡å‹ï¼šConv1D â†’ LSTM â†’ Attention â†’ Dense
- è®­ç»ƒå¹¶æŠŠç»“æœè¿½åŠ å†™å…¥ outputs/metrics.csv
- ä¿å­˜æ¨¡å‹ä¸º outputs/att_cnn_lstm_with_sentiment.h5
"""

from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow.keras.backend as K # ğŸ‘ˆ æ·»åŠ äº† Keras åç«¯å¯¼å…¥


def get_project_root() -> Path:
    # è¿™ä¸ªè„šæœ¬åœ¨ src/3_models/ ä¸‹é¢ï¼Œå¾€ä¸Šä¸¤å±‚å°±æ˜¯é¡¹ç›®æ ¹ç›®å½•
    return Path(__file__).resolve().parents[2]


def create_sequences(X, y, lookback: int):
    """æŠŠäºŒç»´ç‰¹å¾å˜æˆ (samples, timesteps, features) çš„åºåˆ—æ•°æ®"""
    Xs, ys = [], []
    for i in range(len(X) - lookback):
        Xs.append(X[i : i + lookback])
        ys.append(y[i + lookback])
    return np.array(Xs), np.array(ys)


def attention_block(inputs):
    """
    ç®€å• Attention å—ï¼š
    inputs: (batch, timesteps, features)
    """
    score = layers.Dense(1, activation="tanh")(inputs)   # (batch, T, 1)
    weights = layers.Softmax(axis=1)(score)              # (batch, T, 1)
    context = layers.Multiply()([inputs, weights])       # (batch, T, F)
    # ğŸŒŸ ä¿®å¤ï¼šä½¿ç”¨ Lambda å±‚å°è£… K.sum æ¥ä»£æ›¿ tf.reduce_sum
    context = layers.Lambda(lambda x: K.sum(x, axis=1))(context) # (batch, F)
    return context


def main():
    project_root = get_project_root()
    data_path = project_root / "data" / "features" / "merged_features.csv"
    outputs_dir = project_root / "outputs"
    outputs_dir.mkdir(parents=True, exist_ok=True)

    # è·¯å¾„å·²æ›´æ–°
    model_path = outputs_dir / "att_cnn_lstm_with_sentiment.h5"
    metrics_path = outputs_dir / "metrics.csv"

    print(f"ğŸ“¥ Reading merged features from {data_path}")
    df = pd.read_csv(data_path, parse_dates=["date"])

    # ä»·æ ¼ç‰¹å¾
    price_cols = [
        "open",
        "high",
        "low",
        "close",
        "adj_close",
        "volume",
        "return_1d",
        "return_lag_1",
        "return_lag_3",
        "return_lag_7",
        "roll_mean_5",
        "roll_std_5",
        "log_price",
    ]
    price_cols = [c for c in price_cols if c in df.columns]

    # æƒ…ç»ªç‰¹å¾ (å·²åŒ…å«)
    sentiment_cols = [
        "sentiment_mean",
        "sentiment_max",
        "sentiment_min",
        "sentiment_count",
        "sentiment_index",
    ]
    sentiment_cols = [c for c in sentiment_cols if c in df.columns]

    feature_cols = price_cols + sentiment_cols
    target_col = "target_return_1d"

    print("âœ… ä½¿ç”¨çš„ç‰¹å¾åˆ—ï¼ˆPrice + Sentimentï¼‰ï¼š", feature_cols)

    # æŒ‰æ—¶é—´æ’åºï¼Œé˜²æ­¢ä¹±åº
    df = df.sort_values(["date", "ticker"]).reset_index(drop=True)

    X_all = df[feature_cols].values.astype(float)
    y_all = df[target_col].values.astype(float)

    # æ—¶é—´åˆ‡åˆ†ï¼š70% è®­ç»ƒï¼Œ15% éªŒè¯ï¼Œ15% æµ‹è¯•
    n = len(df)
    train_end = int(n * 0.7)
    val_end = int(n * 0.85)

    X_train_raw, y_train_raw = X_all[:train_end], y_all[:train_end]
    X_val_raw, y_val_raw = X_all[train_end:val_end], y_all[train_end:val_end]
    X_test_raw, y_test_raw = X_all[val_end:], y_all[val_end:]

    print(f"ğŸ“Š æ ·æœ¬æ•°ï¼štrain={len(X_train_raw)}, val={len(X_val_raw)}, test={len(X_test_raw)}")

    # æ ‡å‡†åŒ–ï¼ˆæŒ‰è®­ç»ƒé›† fitï¼‰
    scaler = StandardScaler()
    scaler.fit(X_train_raw)

    X_train_scaled = scaler.transform(X_train_raw)
    X_val_scaled = scaler.transform(X_val_raw)
    X_test_scaled = scaler.transform(X_test_raw)

    # ç”Ÿæˆåºåˆ—
    lookback = 20
    X_train_seq, y_train = create_sequences(X_train_scaled, y_train_raw, lookback)
    X_val_seq, y_val = create_sequences(X_val_scaled, y_val_raw, lookback)
    X_test_seq, y_test = create_sequences(X_test_scaled, y_test_raw, lookback)

    num_features = X_train_seq.shape[-1]

    # ğŸŸ£ CNN + LSTM + Attention + Sentiment
    inputs = layers.Input(shape=(lookback, num_features))
    x = layers.Conv1D(filters=32, kernel_size=3, padding="causal", activation="relu")(inputs)
    x = layers.MaxPool1D(pool_size=2)(x)
    x = layers.LSTM(64, return_sequences=True)(x)
    x = attention_block(x)                 # (batch, features)
    x = layers.Dense(32, activation="relu")(x)
    outputs = layers.Dense(1, activation="linear")(x)

    model = keras.Model(inputs, outputs)

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
        loss="mse",
        metrics=["mae"],
    )

    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor="val_loss", patience=5, restore_best_weights=True
        )
    ]

    history = model.fit(
        X_train_seq,
        y_train,
        epochs=50,
        batch_size=64,
        validation_data=(X_val_seq, y_val),
        callbacks=callbacks,
        verbose=1,
    )

    # æµ‹è¯•é›†è¯„ä¼°
    y_pred = model.predict(X_test_seq).ravel()
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    print(f"âœ… Att-CNN-LSTM + Sentiment Test RMSE = {rmse:.6f}, MAE = {mae:.6f}")

    # ä¿å­˜æ¨¡å‹
    model.save(model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")

    # è®°å½•æŒ‡æ ‡
    row = {
        "model_name": "att_cnn_lstm_with_sentiment",
        "use_sentiment": 1, # ğŸ‘ˆ å·²æ›´æ–°ä¸º 1
        "rmse": rmse,
        "mae": mae,
    }

    if metrics_path.exists():
        metrics_df = pd.read_csv(metrics_path)
        metrics_df = pd.concat([metrics_df, pd.DataFrame([row])], ignore_index=True)
    else:
        metrics_df = pd.DataFrame([row])

    metrics_df.to_csv(metrics_path, index=False)
    print(f"ğŸ“ˆ æŒ‡æ ‡å·²å†™å…¥ {metrics_path}")
    print(metrics_df.tail())


if __name__ == "__main__":
    main()